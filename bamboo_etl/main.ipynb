{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a663867f",
   "metadata": {},
   "source": [
    "Timer start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3235948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "start_script_time = perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d71ac4",
   "metadata": {},
   "source": [
    "# Init Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from datetime import date, timedelta, datetime\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "import re\n",
    "import logging\n",
    "from dotenv import dotenv_values\n",
    "import requests\n",
    "import ast\n",
    "from typing import Any\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import execute_values, DictCursor\n",
    "\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "import smtplib\n",
    "import ssl\n",
    "from email.message import EmailMessage\n",
    "\n",
    "ENV_FILE=dotenv_values('.env')\n",
    "\n",
    "# -------------Bamboo PostgreSQL DB config-------------\n",
    "BAMBOO_DB_DSN = ENV_FILE.get('DB_DSN_bamboo')\n",
    "\n",
    "# -------------Google Sheets config-------------\n",
    "GS_CREDS_FILE = Path('bamboo_service_account.json')  # path to JSON google key\n",
    "\n",
    "DASHBOARD_SPREADSHEET_ID = \"1U5ApJgj2_4CWYd_7nV1_LUvUL1aKmU8dB96OlLSd170\"      # id from URL\n",
    "DASHBOARD_WORKSHEET_NAME = \"Лист1\"                         # list name\n",
    "DASHBOARD_SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"] # rights request\n",
    "\n",
    "REQUESTS_PER_MIN_LIMIT = 50\n",
    "SLEEP_BETWEEN_REQUESTS_SEC = 65\n",
    "\n",
    "# -------------API and download period config-------------\n",
    "\n",
    "# today time period\n",
    "# DOWNLOAD_TIME_START = f\"{date.today()} 00:00:00.000000\"\n",
    "# DOWNLOAD_TIME_END = f\"{date.today()} 23:59:59.999999\"\n",
    "\n",
    "#fixed time period \n",
    "DOWNLOAD_TIME_START = '2025-01-01 00:00:00.000000'\n",
    "DOWNLOAD_TIME_END = '2025-06-30 23:59:59.999999'\n",
    "\n",
    "ITRESUME_URL = \"https://b2b.itresume.ru/api/statistics\"\n",
    "ITRESUME_SKILLFACTORY_PARAMS={\n",
    "'client': 'Skillfactory',\n",
    "'client_key':ENV_FILE.get('CLIENT_KEY_Skillfactory'),\n",
    "'start':DOWNLOAD_TIME_START,\n",
    "'end':DOWNLOAD_TIME_END\n",
    "}\n",
    "\n",
    "# -------------GMAIL config-------------\n",
    "BARROCO_SMTP = 'smtp.gmail.com'\n",
    "BARROCO_EMAIL_PORT= 465\n",
    "BARROCO_ACCOUNT_PASSWORD = ENV_FILE.get('GOOGLE_APP_PASS')\n",
    "BARROCO_EMAIL = 'unpocobarroco@gmail.com'\n",
    "BARROCO_EMAIL_NAME = 'Bamboo Project'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab3d51",
   "metadata": {},
   "source": [
    "# DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e044f5",
   "metadata": {},
   "source": [
    "### Classes\n",
    "\n",
    "- **DB_Ops**: communication with DB\n",
    "- **Mail_Server_SSL**: communication with Mail-sevice Server\n",
    "    - SSL connection with SMTP\n",
    "    - Use `with...as` to ensure SMTP-session is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbOps:\n",
    "    \"\"\"\n",
    "    - Encapsulation tools for communication with PostgreSQL database. \n",
    "    - 1 instance = 1 target database (connections are created per operation).\n",
    "    - Takes DSN (Data Source Name) as a string. Example: \"dbname=name user=user password=secret host=localhost port=5432\".\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_dsn:str ):\n",
    "        self.dsn=db_dsn\n",
    "        \n",
    "    def make_table(self, schema:str, table:str, creating_query:str):\n",
    "        \"\"\"Make the new table on the bases of received query. \n",
    "        'with...as' construction ensures closing connection in case of success and rollback if failed.\n",
    "        \n",
    "        Example:\n",
    "        CREATE TABLE IF NOT EXISTS {} (\n",
    "        id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY\n",
    "        , user_id TEXT NOT NULL\n",
    "        , oauth_consumer_key TEXT\n",
    "        , created_at TIMESTAMP NOT NULL\n",
    "        , CONSTRAINT unique_check UNIQUE (user_id, created_at)\n",
    "        )\n",
    "\n",
    "        Args:\n",
    "            schema (str): new table needed schema\n",
    "            table (str): new table name\n",
    "            creating_query (str): full SQL query\n",
    "        \"\"\"\n",
    "        with psycopg2.connect(self.dsn) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                \n",
    "                ready_query=sql.SQL(creating_query).format(sql.Identifier(schema, table))\n",
    "                cur.execute(ready_query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_sql_columns(columns:list|tuple):\n",
    "        \"\"\"Safely compose a dynamic SQL fragment using psycopg2.sql.\n",
    "\n",
    "        Args:\n",
    "            columns: list or tuple\n",
    "\n",
    "        Returns: \n",
    "            Composed SQL fragment\n",
    "            \n",
    "        \"\"\"\n",
    "        return sql.SQL(', ').join(sql.Identifier(col) for col in columns)\n",
    "    \n",
    "    \n",
    "    def insert_unique_data_to_table(self, schema:str, table:str, unique_constraint:str, columns:list|tuple, data:list|tuple):\n",
    "        \"\"\"Safely inserts data into the specified columns. If data is repeated, it will be skipped (ON CONFLICT ON CONSTRAINT ... DO NOTHING).\n",
    "        'with...as' construction ensures closing connection in case of success and rollback if failed.\n",
    "\n",
    "        Args:\n",
    "            schema (str): needed schema\n",
    "            table (str): table name\n",
    "            unique_constraint (str): name of the unique constraint in the target table.\n",
    "            columns: list or tuple\n",
    "            data: sequence of rows\n",
    "\n",
    "        Returns: \n",
    "            int: Number of inserted rows\n",
    "                  \n",
    "        \"\"\"\n",
    "        \n",
    "        if not data:\n",
    "            return 0\n",
    "        \n",
    "        needed_length=len(columns)\n",
    "        for i, row in enumerate(data):\n",
    "            if len(row)!=needed_length:\n",
    "                raise ValueError(f\"{i} row length is not the same as columns count in DB table ({needed_length})\")\n",
    "        \n",
    "        columns_sql=DbOps.make_sql_columns(columns)\n",
    "        \n",
    "        with psycopg2.connect(self.dsn) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "        \n",
    "                query=sql.SQL(\n",
    "                \"\"\"\n",
    "                INSERT INTO {} ({})\n",
    "                VALUES %s\n",
    "                ON CONFLICT ON CONSTRAINT {} DO NOTHING\n",
    "                RETURNING 1\n",
    "                \"\"\").format(sql.Identifier(schema, table), columns_sql, sql.Identifier(unique_constraint))\n",
    "                \n",
    "                returned = execute_values(cur, query, data, page_size=1000, fetch=True)\n",
    "                return len(returned)\n",
    "                \n",
    "            \n",
    "    def query_fetchall_dict(self, query:str):\n",
    "        \"\"\"\n",
    "        - Executing-fetch function for general SQL queries.\n",
    "        - 'with...as' construction ensures closing connection in case of success and rollback if failed.\n",
    "        - No {} or %s are allowed in query\n",
    "\n",
    "        Args:\n",
    "            query (str): full SQL query\n",
    "\n",
    "        Returns:\n",
    "            list[DictRow,...]: List of dict-like rows (DictRow). 1 DictCursor = 1 row in the result table.\n",
    "        \"\"\"\n",
    "\n",
    "        with psycopg2.connect(self.dsn) as conn:\n",
    "            with conn.cursor(cursor_factory=DictCursor) as cur:\n",
    "                \n",
    "                cur.execute(query)\n",
    "                return cur.fetchall()\n",
    "\n",
    "\n",
    "class MailServerSSL:\n",
    "    \"\"\"\n",
    "    - Encapsulates sending email via SMTP over SSL using provided account credentials.\n",
    "    - 1 instance = 1 email account\n",
    "    - Takes smtp_server:str, ssl_port:int, account_password:str, sender_email:str, sender_name:str.\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ssl.create_default_context()\n",
    "    \n",
    "    def __init__(self, smtp_server:str, ssl_port:int, account_password:str, sender_email:str, sender_name:str):\n",
    "        self.account_password=account_password\n",
    "        self.smtp_server = smtp_server\n",
    "        self.ssl_port = ssl_port\n",
    "        self.sender_email = sender_email\n",
    "        self.sender_name=sender_name\n",
    "        \n",
    "    def send_message(self, recipient_email:str, subject:str, message:str):\n",
    "        \"\"\"Send a plain text email.\n",
    "\n",
    "        Args:\n",
    "            recipient_email (str): mail to\n",
    "            subject (str): subject of the message\n",
    "            message (str): message text\n",
    "\n",
    "        Returns:\n",
    "            EmailMessage: Mail message object\n",
    "        \"\"\"\n",
    "\n",
    "        msg = EmailMessage()\n",
    "        \n",
    "        msg['Subject'] = subject\n",
    "        msg['From'] = f\"{self.sender_name} <{self.sender_email}>\"\n",
    "        msg['To'] = recipient_email\n",
    "        msg.set_content(message)\n",
    "        \n",
    "        \n",
    "        with smtplib.SMTP_SSL(self.smtp_server, self.ssl_port, context=MailServerSSL.context) as server:\n",
    "            server.login(self.sender_email, self.account_password)\n",
    "            server.send_message(msg)\n",
    "            \n",
    "        return msg\n",
    "    \n",
    "\n",
    "class GoogleSpreadsheet:\n",
    "    \"\"\"\n",
    "    - Encapsulation tools for communication with Google Spreadsheet. \n",
    "    - 1 instance = 1 Spreadsheet in target account.\n",
    "    - Takes creds_file: Path, scopes: list[str], spreadsheet_id: str.\n",
    "    \n",
    "    Scopes example: [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "    spreadsheet_id: get from URL in browser\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, creds_file: Path, scopes: list[str], spreadsheet_id: str):\n",
    "        \n",
    "        self.creds = Credentials.from_service_account_file(creds_file, scopes=scopes)\n",
    "        self.gc = gspread.authorize(self.creds)         # client\n",
    "        self.sh = self.gc.open_by_key(spreadsheet_id)   # spreadsheet\n",
    "        self.ws = None                                  # worksheet\n",
    "\n",
    "    def select_worksheet(self, worksheet_name: str):\n",
    "        \"\"\"\n",
    "        - Return a worksheet by name, reusing the cached worksheet if available.\n",
    "        - If the cached worksheet title matches the requested name, it is returned directly.\n",
    "\n",
    "        Args:\n",
    "            worksheet_name (str): Name of the worksheet (not id)\n",
    "\n",
    "        Returns: gspread.Worksheet\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.ws and self.ws.title == worksheet_name:\n",
    "            return self.ws\n",
    "        \n",
    "        self.ws = self.sh.worksheet(worksheet_name)\n",
    "        return self.ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4b0c8",
   "metadata": {},
   "source": [
    "###  Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71691697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_logs(log_dir:Path, keep_limit_days:int):\n",
    "    \"\"\"\n",
    "    - Cleans up the old files in target folder.\n",
    "    - Files must contain date in the filename (\"yyyy-mm-dd\").\n",
    "\n",
    "    Args:\n",
    "        log_dir (Path): path object of the target directory\n",
    "        keep_limit_days (int): delete files older then\n",
    "    \"\"\"\n",
    "\n",
    "    today_date=date.today()\n",
    "    keep_limit_date=today_date-timedelta(days=keep_limit_days)\n",
    "\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for p in log_dir.iterdir():\n",
    "        \n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        \n",
    "        filename=p.name\n",
    "        match_date_in_name=re.search(r\"\\d{4}-\\d{2}-\\d{2}\",filename)\n",
    "        \n",
    "        if not match_date_in_name:\n",
    "            continue\n",
    "        iso_date_in_name=date.fromisoformat(match_date_in_name.group())\n",
    "        \n",
    "        if iso_date_in_name > keep_limit_date:\n",
    "            continue\n",
    "        \n",
    "        p.unlink()\n",
    "   \n",
    "        \n",
    "def get_periods(total_seconds:float|int):\n",
    "    \"\"\"Extracts day, hour, minute, and second components from a duration in seconds.\n",
    "\n",
    "    Args:\n",
    "        total_seconds (float | int): number of seconds\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted time periods including zero values.\n",
    "    \"\"\"\n",
    "\n",
    "    days, reminder = divmod(total_seconds, 86400)\n",
    "    hours, reminder = divmod(reminder, 3600)\n",
    "    mins, secs = divmod(reminder, 60)\n",
    "\n",
    "    return {'d': days, 'h': hours, 'm': mins, 's': secs}\n",
    "        \n",
    "\n",
    "def string_to_dict(string:str):\n",
    "    \"\"\"Converts string to dict format.\n",
    "\n",
    "    Args:\n",
    "        string (str): string\n",
    "\n",
    "    Returns:\n",
    "        dict: dict with content (if conversion succeeded) or empty dict (if failed)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        res = ast.literal_eval(string)\n",
    "        \n",
    "        if not isinstance(res, dict):\n",
    "            logging.warning(\"Converted object format is not dict\")\n",
    "            return {}           \n",
    "        \n",
    "        return res\n",
    "        \n",
    "    except Exception:\n",
    "        logging.warning(f\"Not able to convert passback_params to dict format\")\n",
    "        return {}\n",
    "        \n",
    "\n",
    "def convert_to_datetime(string:str):\n",
    "    \"\"\"Validates the string content: returns datetime object if iso-format, if fail - raise ValueError.\n",
    "\n",
    "    Args:\n",
    "        string (str): string\n",
    "\n",
    "    Raises:\n",
    "        ValueError: \"Date conversion Error\"\n",
    "\n",
    "    Returns:\n",
    "        datetime: Datetime object\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return datetime.fromisoformat(string)\n",
    "    \n",
    "    except Exception:\n",
    "        logging.warning(f\"Not able to convert {string} to ISO-format string.\")\n",
    "        raise ValueError(\"Date conversion Error\")\n",
    "\n",
    "\n",
    "def validate_return_value(value:Any, accepted_vals: tuple | None = None, class_or_tuple = None):\n",
    "    \"\"\"Validates the value type and allowed values. Raises Validation Error if fails.\n",
    "\n",
    "    Args:\n",
    "        value (Any): value\n",
    "        accepted_vals (tuple | None, optional): accepted vals tuple. Defaults to None.\n",
    "        class_or_tuple (_type_, optional): expected type or tuple of types. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: \"Value Validation Error\" or \"Type Validation Error\"\n",
    "\n",
    "    Returns:\n",
    "        Any: Target value (if the validation succeeds)\n",
    "    \"\"\"\n",
    "    \n",
    "    if class_or_tuple is not None and not isinstance(value, class_or_tuple):\n",
    "        logging.warning(f\"Type Validation Error: {value} doesn`t fit {class_or_tuple}\")\n",
    "        raise ValueError(\"Type Validation Error\")\n",
    "    \n",
    "    if accepted_vals is not None and (value not in accepted_vals):\n",
    "        logging.warning(f\"Value Validation Error: {value} is not in {accepted_vals}\")\n",
    "        raise ValueError (\"Value Validation Error\")\n",
    "    \n",
    "    return value\n",
    "\n",
    "\n",
    "def hook(obj: dict):\n",
    "    \"\"\"JSON hook that transforms data to the target structure and validates contained values.\n",
    "\n",
    "    Args:\n",
    "        obj (dict): JSON payload received from the webhook.\n",
    "\n",
    "    Returns:\n",
    "        dict: Normalized payload as a plain dictionary, or None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        passback = string_to_dict(obj['passback_params'])\n",
    "        \n",
    "        # extract the vals according the correct structure as it is\n",
    "        user_id = obj['lti_user_id']\n",
    "        oauth_consumer_key = passback.get('oauth_consumer_key', None)\n",
    "        lis_result_sourcedid = passback.get('lis_result_sourcedid', None)\n",
    "        lis_outcome_service_url = passback.get('lis_outcome_service_url', None)\n",
    "        is_correct = obj['is_correct']\n",
    "        attempt_type = obj['attempt_type']\n",
    "        created_at = obj['created_at']\n",
    "        \n",
    "        # validate and put to the structure\n",
    "        plain_dict = {\n",
    "            'user_id':validate_return_value(user_id, class_or_tuple=str), # строковый айди пользователя\n",
    "            'oauth_consumer_key': validate_return_value(oauth_consumer_key, class_or_tuple=(str, type(None))), # уникальный токен клиента\n",
    "            'lis_result_sourcedid': validate_return_value(lis_result_sourcedid, class_or_tuple=(str, type(None))), # ссылка на блок, в котором находится задача в ЛМС\n",
    "            'lis_outcome_service_url': validate_return_value(lis_outcome_service_url, class_or_tuple=(str, type(None))), # URL адрес в ЛМС, куда мы шлем оценку\n",
    "            'is_correct': validate_return_value(is_correct, accepted_vals=(0, 1, None), class_or_tuple=(int, type(None))), # была ли попытка верной (null, если это run)\n",
    "            'attempt_type': validate_return_value(attempt_type, accepted_vals=('run', 'submit'), class_or_tuple=str), # ран или сабмит\n",
    "            'created_at': convert_to_datetime(created_at)  # дата и время попытки\n",
    "            }\n",
    "        \n",
    "        return plain_dict\n",
    "    \n",
    "    except Exception:\n",
    "        logging.warning(f\"Above fail araised in: created_at {obj.get('created_at', 'Empty')}, user_id {obj.get('lti_user_id', 'Empty')}\")\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6381d",
   "metadata": {},
   "source": [
    "# Bootstrap\n",
    "- Log directory manage\n",
    "- Log files cleanup\n",
    "- Logging settings: force=True - гарантия создания нового файла при смене даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"logs\")\n",
    "keep_limit_days=3\n",
    "\n",
    "cleanup_logs(log_dir, keep_limit_days)\n",
    "\n",
    "log_file = log_dir / f\"logs ({date.today()}).txt\"\n",
    "logging.basicConfig(filename=log_file,level=logging.INFO, filemode='a', format=\"%(asctime)s %(name)s %(levelname)s: %(message)s\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054ea5e",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa063041",
   "metadata": {},
   "source": [
    "## Extracting API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c871da",
   "metadata": {},
   "source": [
    "### Downloading data by API\n",
    "- using extra visual check (first_row_data) for further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"====================== New ETL started ======================\")\n",
    "logging.info(\"Start downloading data by API\")\n",
    "\n",
    "try:\n",
    "    \n",
    "    r=requests.get(ITRESUME_URL, params=ITRESUME_SKILLFACTORY_PARAMS, timeout=None)\n",
    "    \n",
    "    r.raise_for_status()\n",
    "    first_row_data=r.json()\n",
    "    rows_income_cnt=len(first_row_data)\n",
    "    \n",
    "    logging.info(f\"Downloading data by API completed successfully. Code {r.status_code}. {rows_income_cnt} rows total\")\n",
    "    \n",
    "except Exception:\n",
    "    logging.exception(\"Invalid get-parameters or read timed out\")\n",
    "    raise\n",
    "\n",
    "\n",
    "first_row_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f6b5a",
   "metadata": {},
   "source": [
    "## Data transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d49a00",
   "metadata": {},
   "source": [
    "### Validate and transform of downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25917c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"----- Raw data transformation started -----\")    \n",
    "\n",
    "try:    \n",
    "    \n",
    "    user_solutions_structured=r.json(object_hook=hook)\n",
    "    logging.info(\"Transformation stage 1 (rows transformation and validation) completed\")   \n",
    "    \n",
    "except Exception:\n",
    "    logging.exception(\"Transformation stage 1 (rows transformation and validation) failed\")\n",
    "    raise\n",
    "\n",
    "user_solutions_structured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f499fb",
   "metadata": {},
   "source": [
    "### Get rid of the skipped rows (\"None`s\") in the list of user solutions \n",
    "- clearing nearly in-place to safe RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378dfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    user_solutions_structured[:]=[x for x in user_solutions_structured if x is not None]\n",
    "    logging.info(\"Transformation stage 2 (non-working rows clearing) completed\")\n",
    "\n",
    "    rows_transformed_cnt=len(user_solutions_structured)\n",
    "    \n",
    "    logging.info(f\"All stages of raw data transformation completed: {rows_transformed_cnt} rows of {rows_income_cnt} done\")\n",
    "\n",
    "except Exception:\n",
    "    logging.exception(\"Transformation stage 2 (non-working rows clearing) failed\")\n",
    "    raise\n",
    "\n",
    "\n",
    "user_solutions_structured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37c59d",
   "metadata": {},
   "source": [
    "## Load to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fda12f",
   "metadata": {},
   "source": [
    "### Creating a table in DB if doesn`t exist + put the info into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0597ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"----- Start communication with Database -----\")\n",
    "\n",
    "try:\n",
    "\n",
    "    bamboo=DbOps(BAMBOO_DB_DSN)\n",
    "\n",
    "    table_schema ='public'\n",
    "    table_name = 'solutions'\n",
    "    unique_constraint = 'unique_check'\n",
    "    target_in_log=f\"{table_schema}.{table_name}\"\n",
    "    \n",
    "    creating_query=\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {} (\n",
    "        id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY\n",
    "        , user_id TEXT NOT NULL\n",
    "        , oauth_consumer_key TEXT\n",
    "        , lis_result_sourcedid TEXT\n",
    "        , lis_outcome_service_url TEXT\n",
    "        , is_correct INT CHECK (is_correct in (0, 1) OR is_correct is NULL)\n",
    "        , attempt_type TEXT CHECK (attempt_type in ('run', 'submit')) NOT NULL\n",
    "        , created_at TIMESTAMP NOT NULL\n",
    "        , CONSTRAINT unique_check UNIQUE (user_id, created_at)\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    bamboo.make_table(table_schema, table_name, creating_query)\n",
    "    logging.info(f\"[{target_in_log}] Checking/creating table completed successfully\")\n",
    "    \n",
    "    columns_for_inserting=('user_id', 'oauth_consumer_key', 'lis_result_sourcedid', 'lis_outcome_service_url', 'is_correct', 'attempt_type', 'created_at')\n",
    "      \n",
    "    tuples_to_insert=[(\n",
    "        row[\"user_id\"],\n",
    "        row[\"oauth_consumer_key\"],\n",
    "        row[\"lis_result_sourcedid\"],\n",
    "        row[\"lis_outcome_service_url\"],\n",
    "        row[\"is_correct\"],\n",
    "        row[\"attempt_type\"],\n",
    "        row[\"created_at\"]\n",
    "    ) for row in user_solutions_structured]\n",
    "    rows_prepared_cnt = len(tuples_to_insert)\n",
    "    logging.info(f\"[{target_in_log}] Insert-tuples list preparing completed successfully\") \n",
    "    \n",
    "    inserted_rows_cnt = bamboo.insert_unique_data_to_table(table_schema, table_name, unique_constraint, columns_for_inserting, tuples_to_insert)\n",
    "    logging.info(f\"[{target_in_log}] Inserting data to DB-table completed. {inserted_rows_cnt} new rows of {rows_prepared_cnt} prepared were inserted\")\n",
    "    \n",
    "    logging.info(\"Communication with Database completed successfully\") \n",
    "\n",
    "except Exception:\n",
    "    logging.exception(\"Communication with Database failed. Transaction (if started) was rolled back automatically.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2848f",
   "metadata": {},
   "source": [
    "## SQL Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b887b9b",
   "metadata": {},
   "source": [
    "### Prepare aggregated data dict, using DB\n",
    "- Always aggregate all data because:\n",
    "    1) the previous data can be added the following day because of the timezone\n",
    "    2) worksheet can be changed by everyone, BD - can`t, so need to update correct info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"----- Direct database analytics starts -----\")\n",
    "\n",
    "try:    \n",
    "\n",
    "\tdb_dic_list= bamboo.query_fetchall_dict(\n",
    "\t\"\"\" \n",
    "\tWITH \n",
    "\t\tfirst_seen AS (\n",
    "\t\t\tSELECT \n",
    "\t\t\t\tmin(date(created_at)) AS study_date\n",
    "\t\t\t\t, user_id\n",
    "\t\t\tFROM public.solutions\n",
    "\t\t\tGROUP BY user_id\n",
    "\t\t),\n",
    "\t\t\n",
    "\t\tnew_users AS (\n",
    "\t\t\tSELECT\n",
    "\t\t\t\tstudy_date\n",
    "\t\t\t\t, count(user_id) AS new_users_cnt\n",
    "\t\t\tFROM first_seen\n",
    "\t\t\tGROUP BY study_date\n",
    "\t\t),\n",
    "\n",
    "\t\tsolutions_metrics AS (\n",
    "\t\t\tSELECT \n",
    "\t\t\t\tdate(created_at) AS study_date\n",
    "\t\t\t\t, count(*) AS attempts_per_day_total\n",
    "\t\t\t\t, count(*) FILTER(WHERE is_correct=1) AS correct_attempts_per_day\n",
    "\t\t\t\t, count(DISTINCT user_id) AS unique_users_per_day\t\n",
    "\t\t\tFROM public.solutions\n",
    "\t\t\tGROUP BY study_date\n",
    "\t\t\tORDER BY study_date\n",
    "\t\t)\n",
    "\t\t\t\n",
    "\tSELECT \n",
    "\t\tto_char(sm.study_date, 'yyyy-mm-dd') AS study_date\n",
    "\t\t, attempts_per_day_total\n",
    "\t\t, correct_attempts_per_day\n",
    "\t\t, unique_users_per_day\t\n",
    "\t\t, COALESCE(nu.new_users_cnt, 0) AS new_users_per_day\n",
    "\tFROM solutions_metrics sm\n",
    "\tLEFT JOIN new_users nu USING(study_date)\n",
    "\tORDER BY study_date\n",
    "\t\"\"\"\n",
    "\t)\n",
    "\t\n",
    "\tfetch_rows_cnt = len(db_dic_list)\n",
    "\tlogging.info(f\"Aggregated report ({fetch_rows_cnt} rows) for Google Spreadsheets Dashboard was successfully fetched by using SQL query to DB\")\n",
    "\n",
    "except Exception:\n",
    "    logging.exception(\"SQL query construction failed\")\n",
    "    raise\n",
    "\n",
    "\n",
    "db_dic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d403dd",
   "metadata": {},
   "source": [
    "## Communucation with Google Spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0896bd",
   "metadata": {},
   "source": [
    "### Sheet headers prepare directly in WS\n",
    "- get existing Google ws headers, \n",
    "- check if headers are the same as in DB, add new headers from DB if needed (nothing deletes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f55e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"----- Start communucation with Google Spreadsheet -----\")\n",
    "\n",
    "try:\n",
    "     \n",
    "    ws = GoogleSpreadsheet(GS_CREDS_FILE, DASHBOARD_SCOPES, DASHBOARD_SPREADSHEET_ID).select_worksheet(DASHBOARD_WORKSHEET_NAME)\n",
    "    \n",
    "    logging.info(\"Connecting to Google Spreadsheet is stable\")\n",
    "\n",
    "except Exception:\n",
    "    logging.exception(\"Using of Google Sheets credentials failed\")\n",
    "\n",
    "try:\n",
    "    \n",
    "    if fetch_rows_cnt==0:\n",
    "        logging.warning(\"No rows for dashboard. Skipping Worksheet update\")\n",
    "        \n",
    "    else:\n",
    "        ws_headers=ws.row_values(1)\n",
    "        query_headers=db_dic_list[0].keys()\n",
    "\n",
    "        add_headers=[header for header in query_headers if header not in ws_headers]\n",
    "            \n",
    "        if add_headers:\n",
    "            ws_headers.extend(add_headers)\n",
    "            ws.update(range_name='1:1', values=[ws_headers], value_input_option=\"USER_ENTERED\")\n",
    "            \n",
    "        logging.info(\"Worksheet headers preparing completed successfully\")\n",
    "\n",
    "except Exception:\n",
    "    logging.exception(\"Headers preparing in Worksheet failed\")\n",
    " \n",
    "        \n",
    "ws_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cdbe2d",
   "metadata": {},
   "source": [
    "### Add aggregated data to WS in Google Sheets\n",
    "- If date exists- row replace, if not - row append. \n",
    "- Can find the date-column dinamicaly. \n",
    "- The row structure automatically fits the Google ws neww structure.\n",
    "- Lines qty doesn`t validate\n",
    "- Columns width doesn`t change\n",
    "- Columns with same name doesn`t validate - just change both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    dates_in_ws=ws.col_values(ws_headers.index('study_date')+1)\n",
    "\n",
    "    for i, dic in enumerate(db_dic_list):\n",
    "        \n",
    "        if i>0 and i % REQUESTS_PER_MIN_LIMIT==0:\n",
    "            logging.info(f\"Quota of 'Write requests' exceeded limit {REQUESTS_PER_MIN_LIMIT}. Sleeping {SLEEP_BETWEEN_REQUESTS_SEC} seconds\")\n",
    "            sleep(SLEEP_BETWEEN_REQUESTS_SEC)\n",
    "        \n",
    "        row_to_add=[dic.get(key) for key in ws_headers]\n",
    "        row_date=dic['study_date']\n",
    "        \n",
    "        if row_date in dates_in_ws:\n",
    "            ws.update(range_name=f\"A{dates_in_ws.index(row_date)+1}\", values=[row_to_add], value_input_option=\"USER_ENTERED\")\n",
    "            \n",
    "        else:\n",
    "            ws.append_row(row_to_add, value_input_option=\"USER_ENTERED\")\n",
    "            \n",
    "    logging.info(\"Aggregated analytics appending to Google Worksheet completed successfully\")\n",
    "    logging.info(\"Communucation with Google Spreadsheet completed successfully\")\n",
    "\n",
    "except Exception:\n",
    "    logging.exception(\"Failed to append analytical report data to Google Worksheet\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f691f",
   "metadata": {},
   "source": [
    "## Communucation with SMTP Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b84c32",
   "metadata": {},
   "source": [
    "### Send digest message via email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"----- Start communucation with SMTP Server -----\")\n",
    "\n",
    "try:\n",
    "        \n",
    "    gmail = MailServerSSL(BARROCO_SMTP, BARROCO_EMAIL_PORT, BARROCO_ACCOUNT_PASSWORD, BARROCO_EMAIL, BARROCO_EMAIL_NAME)\n",
    "\n",
    "    message=f\"\"\"\n",
    "    Dear all,\n",
    "    \n",
    "    Thanks for following our team digest!\n",
    "    \n",
    "    Todays ETL script was runned successfully:\n",
    "    Rows extracted by API: {rows_income_cnt}\n",
    "    Rows validated: {rows_transformed_cnt}\n",
    "    Rows prepared to insert to DB: {rows_prepared_cnt}\n",
    "    New rows inserted to DB: {inserted_rows_cnt}\n",
    "    \n",
    "    The dashboard Google Sheet was also updated successfully.\n",
    "    \n",
    "    BR,\n",
    "    Bamboo team\n",
    "    \"\"\"\n",
    "    logging.info(\"Email digest message preparing completed successfully\")\n",
    "    \n",
    "    msg = gmail.send_message('fluxor@atomicmail.io', \"Work done\", message)\n",
    "    logging.info(\"Email digest message was sent successfully\")\n",
    "    \n",
    "    logging.info(\"Communucation with SMTP Server completed successfully\")\n",
    "\n",
    "except Exception:\n",
    "    logging.exception(\"Communucation with SMTP Server failed\")\n",
    "    raise    \n",
    "\n",
    "    \n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40193603",
   "metadata": {},
   "source": [
    "Timer stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"----- ETL script finished successfully -----\")\n",
    "logging.info(f\"Rows extracted by API: {rows_income_cnt}\")\n",
    "logging.info(f\"Rows validated: {rows_transformed_cnt} \")\n",
    "logging.info(f\"Rows prepared to insert to DB: {rows_prepared_cnt}\")\n",
    "logging.info(f\"New rows inserted to DB: {inserted_rows_cnt} \")\n",
    "\n",
    "extract_seconds = round((datetime.fromisoformat(DOWNLOAD_TIME_END) - datetime.fromisoformat(DOWNLOAD_TIME_START)).total_seconds())\n",
    "extracted_periods = get_periods(extract_seconds)\n",
    "\n",
    "e_seconds = f\"{round(extracted_periods['s'], 2)} seconds\"\n",
    "e_minutes = '' if extracted_periods['m']==0 else f\"{extracted_periods['m']} minutes, \"\n",
    "e_hours= '' if extracted_periods['h']==0 else f\"{extracted_periods['h']} hours, \"\n",
    "e_days= '' if extracted_periods['d']==0 else f\"{extracted_periods['d']} days, \"\n",
    "logging.info(f\"API Extracted period: {e_days}{e_hours}{e_minutes}{e_seconds}\")\n",
    "\n",
    "\n",
    "finish_script_seconds = perf_counter() - start_script_time\n",
    "finish_periods = get_periods(finish_script_seconds)\n",
    "\n",
    "f_seconds = f\"{round(finish_periods['s'], 2)} seconds\"\n",
    "f_minutes = '' if finish_periods['m']==0 else f\"{finish_periods['m']} minutes, \"\n",
    "f_hours= '' if finish_periods['h']==0 else f\"{finish_periods['h']} hours, \"\n",
    "f_days= '' if finish_periods['d']==0 else f\"{finish_periods['d']} days, \"\n",
    "logging.info(f\"Script total run time: {f_days}{f_hours}{f_minutes}{f_seconds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
